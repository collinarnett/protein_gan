{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.PDB import *\n",
    "import multiprocessing as mp\n",
    "import h5py\n",
    "import logging\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biopython creates warnings for chains that are discontinuous. I recommend turning them off.\n",
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "warnings.simplefilter('ignore', BiopythonWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='data_generation.log',level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "#----------------\n",
    "# Requires absolute path\n",
    "test_path = pathlib.Path(\"/home/collin/protein_gan/data/test/\")\n",
    "train_path = pathlib.Path(\"/home/collin/protein_gan/data/train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_matrix(matrix):\n",
    "    if (len(matrix)>=RES):\n",
    "        for n in range(1,int(floor(len(matrix)/RES))):\n",
    "            # Creating RES x RES matrices by traversing the spine of input matrix\n",
    "            matrix_chunk = matrix[RES*(n-1):RES*n, RES*(n-1):RES*n]\n",
    "            yield matrix_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_matrix(residues):\n",
    "    \"\"\"Returns a matrix of distances between residues of the same chain.\"\"\"\n",
    "    size = len(residues)\n",
    "    answer = np.zeros((size, size), np.float)\n",
    "    for row, residue_one in enumerate(residues):\n",
    "        for col, residue_two in enumerate(residues):\n",
    "            answer[row, col] = residue_one[\"CA\"] - residue_two[\"CA\"]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_maps(files):\n",
    "    \"\"\"\n",
    "    Generate specified resolution a-carbon maps given a input directory\n",
    "    \"\"\"\n",
    "    # Create A chain maps as matrices\n",
    "    parser = PDBParser()\n",
    "    io = PDBIO()\n",
    "    # Get the initial structure of the protein\n",
    "    try:    \n",
    "        structure = parser.get_structure('X', files)\n",
    "        for models in structure:\n",
    "            residues = Selection.unfold_entities(models['A'], 'R')\n",
    "            ca_residues = [residue for residue in residues if 'CA' in residue]\n",
    "            distance_matrix = calc_dist_matrix(ca_residues)\n",
    "            return list(split_matrix(distance_matrix))\n",
    "    except ValueError as err:\n",
    "        logging.error(f'ValuError file :{files}, Error is:{err}')\n",
    "    except TypeError as err:\n",
    "        logging.error(f'TypeError file :{files}, Error is:{err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(files, desc):\n",
    "    \"\"\"\n",
    "    Clean the generated maps using all cores in the process\n",
    "    \"\"\"\n",
    "    p = mp.Pool(maxtasksperchild=3)\n",
    "    pdbs = [file for file in files.glob(\"*.pdb\")]\n",
    "    r = list(tqdm(p.imap(generate_maps, pdbs), total=len(pdbs), desc=desc))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(result):\n",
    "    test_len = [len(x) for x in result]\n",
    "    plt.hist(test_len)\n",
    "    plt.show()\n",
    "    plt.imshow(result[5], cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 16\n",
    "test_16 = main(test_path, \"test_16\")\n",
    "test_16 = [item for sublist in test_16 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('test_16', data=test_16, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 16\n",
    "train_16 = main(train_path, \"train_16\")\n",
    "train_16 = [item for sublist in train_16 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('train_16', data=train_16, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 64\n",
    "test_64 = main(test_path, \"test_64\")\n",
    "test_64 = [item for sublist in test_64 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('test_64', data=test_64, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 64\n",
    "train_64 = main(train_path, \"train_64\")\n",
    "train_64 = [item for sublist in train_64 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('train_64', data=train_64, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 128\n",
    "test_128 = main(test_path, \"test_128\")\n",
    "test_128 = [item for sublist in test_128 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('test_128', data=test_128, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 128\n",
    "train_128 = main(train_path, \"train_128\")\n",
    "train_128 = [item for sublist in train_128 if sublist for item in sublist]\n",
    "with h5py.File('dataset.hdf5', 'a') as f:\n",
    "    f.create_dataset('train_128', data=train_128, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
